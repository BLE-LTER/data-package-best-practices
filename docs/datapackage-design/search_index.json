[
["index.html", "Best Practices for Data Package Design Best Practice Recommendations for Data Package Design", " Best Practices for Data Package Design Environmental Data Initiative 2020-06-01 Best Practice Recommendations for Data Package Design Members of the working group developing these documents: S. Beaulieu, R. Brown, J. Downing, S. Elmendorf, H. Garritt, G. Gastil-Buhl, C. Gries, J. Hollingsworth, H.-Y. Hsieh, L. Kui, M. Martin, G. Maurer, A. Nguyen, J. Porter, M. Servilla, T. Whiteaker Considerations for a well designed data package including special cases based on data type, format, or acquisition method. Examples are images, documents, raw data stored in other repositories. A data package is the published unit of data and metadata together. It may contain one or more entities, such as csv tables, GIS shape files or geodatabases, processing or modeling code, other documents (pdf, jpg, zip). Data File Type It is recommended to use a standard file format that is likely to still be machine readable in the future. That is, preferably not a binary format that needs specialized software for interpretation. E.g., Excel spreadsheets should be archived as csv text files and Word documents as pdf. Clearly these recommendations will change and depend on community practices. Entities to Include in a Data Package A data package should be a logical, coherent unit. This may be one data package with all data collected within one project or many data packages each containing one data table. The questions to ask are: Are all data necessary to understand each table? E.g., air temperature may be important to understand insect behaviour data. Alternatively, does it make more sense to publish each data stream separately because they can be made better discoverable with more specific metadata. E.g., Will the air temperature data be useful when combined with other air temperature data to answer a very different question. Processing code, PDF of protocols, fieldsheets, etc. may be included if they are important to understand and reuse the data. Specific recommendations for archiving code, images as data, model based data. Splitting and Lumping of Data The overall goal should be to make data useful to somebody who was not involved in collecting them. Hence, a balance should be struck between too many data units or too large a data unit. E.g., most long term timeseries data should be kept combined into one file not annual increments. Unless that file becomes so large that it will difficult to handle via internet download or computing power of the user. Clearly, this is changing rapidly and no hard numbers can be given. Data File Format Most ecological data are organized in wide table format, also called matrix style, with each column being of one data type and one parameter. E.g., sampling sites, sampling date, air temperature, relative humidity, solar radiation. This organization lends itself best for documentation in the Ecological Metadata Language (EML) as each column can be described. In some cases, however, a data curator may decide to format the data in a long table format (also called key-value pair tables). In this case there is a limited number of columns, one of which contains the name of the parameter while the next one contains the value for this paramter. E.g., sampling site, sampling date, parameter name, value. This format is preferable when there is some variation in which parameters are sampled at a give site and date, i.e., the matrix formated table would have many empty cells. Data in Other Repositories It makes sense to provide cross references to data in other repositories within a data package if they become better discoverable or additional contextual data are provided. However, duplicating data without clearly referring to the original source is not recommended. Special recommendations are made for genomics data Very Large Datasets Most repositories have an upper limit as to how large an archived dataset may be. The Environmental Data Initiative offers to option to store such data off line, i.e., not downloadable from the data access portal. Metadata provide contact information for requesting such data. "],
["key-value-pair-tables.html", "1 Key-Value Pair Tables 1.1 Introduction 1.2 Recommendations for data packages", " 1 Key-Value Pair Tables 1.1 Introduction The Key-Value data model - also known as Attribute-Value or “string of pearls” data model - is widely used for certain kinds of observational data where the more conventional matrix type model would cause many empty cells. This data model treats each point observation as a single record. Typically for environmental science data, each record contains fields for location, time, variable name (or attribute name, the “key”) and value, plus various flags for methods, data quality etc. For ecologists, this data model is especially useful for (1) biodiversity and (2) sensor data that may otherwise require very wide tables in a matrix format, and/or have many empty cells e.g., when species are not regularly observed or sensors are not employed consistently. The advantages of the Attribute - Value data model are its flexibility and efficiency. However datasets in this format are not easily described in the current version of EML. The basic problem is that the values in the value field do not necessarily share the same descriptions (e.g. data type, collection methods, unit, and precision). The only way you can constrain the typing and precision with the EML attribute model is if all values in a column share these. 1.2 Recommendations for data packages 1.2.1 Biodiversity data If all of the values in a given table share the same units (e.g., presence/absence, count, percent abundance), then the table should be represented in this Attribute – Value format and described accurately in EML. If not, then the data should be represented in matrix format, or possibly several matrices if the table is unreasonably wide. The ecocomDP model developed by EDI for harmonizing ecological community survey data is a key-value style table. For more information on that model, see Recommendations for ecological community surveys. 1.2.2 Sensor data The Attribute - Value format is especially efficient for sensor data and variations of this data model have been developed in communities handling large volumes of sensor data (e.g., ODM or Observation Data Model from CUAHSI). For more information for using that model in EML data packages, see Recommendations for meteorology and hydrology data.html. For a single site, this data model is useful when sensors are frequently changed or redeployed or when several different sets of sensor deployments exist. To accurately describe in EML (e.g., typing, precision), it is recommended that each data table contain only the Attribute – Value formatted values for a single sensor. A single data package with data from multiple sensors, can have values for each sensor in separate entities (data tables). Most of these data models describe the concept of a single “data stream,” which may be used to achieve the here recommended approach dynamically. "],
["code-in-edi.html", "2 Code in EDI 2.1 Introduction 2.2 Recommendations for data packages 2.3 Resources", " 2 Code in EDI Contributors: An Nguyen, Tim Whiteaker 2.1 Introduction This document describes best practices for archiving software, code, or scripts, such as a simulation model, data visualization package, or data munging scripts. This document was written for the intended audience of a Long Term Ecological Research (LTER) information manager (IM), though it is applicable to anyone operating in the context of a larger ecological research program. We assume that the target archive is designed to handle ecological data, and that a given archive package will include metadata written in Ecological Metadata Language (EML) format. We refer to the Environmental Data Initiative (EDI) as an example archive, though the same practices could be applied to other similar repositories. 2.1.1 Background While software can be described in detail using the EML &lt;software&gt; tree, there exists a project called CodeMeta which is designed with software metadata in mind. In fact, PASTA currently only accepts EML which have &lt;dataset&gt; as the child element beneath the root &lt;eml&gt; element. Therefore, one of the key recommendations in this document is to move away from the &lt;software&gt; node in favor of including a CodeMeta file when archiving software or code in EDI. We recognize that there are other candidate archives for code. For example, Zenodo, a popular DOI-minting all-purpose repository, can conveniently archive a specific version of code in a GitHub repository. The best practices in this document cover both archiving code in EDI and referencing code archived elsewhere. The intention of these recommendations is to make research based on modeling or software more transparent rather than achieve exact reproducibility, i.e., provide sufficient documentation so that a knowledgeable person can understand algorithms, programming decisions, and their ramifications for the results, rather than run the model and obtain the same results. Of course, if software can be easily packaged with data so that an end user can easily run the code, all the better. 2.2 Recommendations for data packages 2.2.1 Software/code in EDI When deciding whether to and how to archive software or code, consider the following: Is it a model and/or a model-based dataset? Also see the best practices for archiving model-based datasets. How likely is it that the code will be well maintained into the future? E.g., code packages submitted to established code repositories may stay there only while they comply with all testing requirements. That is, they will need regular updates whenever dependencies are updated. E.g.: the R package repository CRAN might remove packages that are no longer actively maintained. If that commitment to code maintenance is unlikely, such a package should be archived. Should the code be archived as a separate package or with the data? If the code is used to generate several independent datasets it should be archived as a separate package. The software authors wishing to place it under a different license from that of the associated data, or to obtain a DOI for only the code, may be reason to separate code and data packages. If deciding to package code separately, it may be archived on EDI, or another repository such as Zenodo or Figshare (see section 2 on external software). In most other cases, it is recommended to archive code and data together on EDI for context. Large community software packages may undergo significant updates and it may make sense to archive the code of a certain version with the data for transparency reasons. Consider whether prior versions of a software package are available wherever that model is distributed. When archiving software, we strongly recommend including a user guide with installation and usage instructions if such would not already be apparent to the typical user. Take into account that the user might not have access to certain inputs that the software/scripts require. Include when feasible at least some example data, and configure the script so that it is ready to run with the example data. 2.2.2 Documenting software/code Include the code as an otherEntity. Although a well documented human readable text format of the code is preferred, in case of multiple scripts, and/or where directory structure is important, a zip or tarball may be used. For the formatName element in EML, we recommend using MIME types if available for your code format. You may not find a perfect fit for your file type, so just do the best you can. For the entityType element, use a simple name for the entity type such as “R markdown”. For more suggestions on formatName and entityType, see the best practices for EML entity metadata. Example 1: EML otherEntity snippet for a script file. &lt;otherEntity&gt; &lt;entityName&gt;R script to process CTD data&lt;/entityName&gt; &lt;entityDescription&gt;Annotated RMarkdown script to process, calibrate, and flag raw CTD data.&lt;/entityDescription&gt; &lt;physical&gt; &lt;objectName&gt;BLE_LTER_CTD_QAQC.Rmd&lt;/objectName&gt; &lt;size unit=&quot;byte&quot;&gt;9674&lt;/size&gt; &lt;authentication method=&quot;MD5&quot;&gt;8547b7a63fcf6c1f0913a5bd7549d9d1&lt;/authentication&gt; &lt;dataFormat&gt; &lt;externallyDefinedFormat&gt; &lt;formatName&gt;text/markdown&lt;/formatName&gt; &lt;/externallyDefinedFormat&gt; &lt;/dataFormat&gt; &lt;/physical&gt; &lt;entityType&gt;R markdown&lt;/entityType&gt; &lt;/otherEntity&gt; CodeMeta Include a CodeMeta JSON file for all code that is archived in EDI. The CodeMeta file should be named “codemeta.json” and listed as an EML otherEntity. The formatName should be “application/json”, and the entityDescription should specify CodeMeta. Do include some kind of license to limit your liability. Generally, this should match the license on the overall package; however, if the package has a mix of data and code, and they each fall under different licenses, then separating them might be advisable, but ultimately we leave that decision up to you. For unnamed projects, e.g., one-off scripts for data processing, analysis, and/or visualisation, a CodeMeta might appear to be overkill; however, CodeMeta files are quick and simple to generate, and we recommend the below bare minimum. If there are multiple scripts each in their own otherEntity tag, we recommend aggregating information about them into one codemeta.json. Example 2: Minimum recommended codemeta.json example for unnamed projects. { &quot;@context&quot;: [&quot;https://doi.org/10.5063/schema/codemeta-2.0&quot;, &quot;http://schema.org&quot; ], &quot;@type&quot;: &quot;SoftwareSourceCode&quot;, &quot;description&quot;: &quot;RMarkdown script to calibrate and flag raw CTD data.&quot;, &quot;author&quot;: { &quot;@type&quot;: &quot;Person&quot;, &quot;givenName&quot;: &quot;Christina&quot;, &quot;familyName&quot;: &quot;Bonsell&quot;, &quot;email&quot;: &quot;cbonsell@utexas.edu&quot;, &quot;@id&quot;: &quot;https://orcid.org/0000-0002-8564-0618&quot; }, &quot;keywords&quot;: [&quot;calibration&quot;, &quot;CTD&quot;, &quot;RMarkdown&quot;], &quot;license&quot;: &quot;https://unlicense.org/&quot;, &quot;dateCreated&quot;: &quot;2013-10-19&quot;, &quot;programmingLanguage&quot;: { &quot;@type&quot;: &quot;ComputerLanguage&quot;, &quot;name&quot;: &quot;R&quot;, &quot;version&quot;: &quot;3.6.2&quot;, &quot;url&quot;: &quot;https://r-project.org&quot; } } Example 3: sample otherEntity metadata for example 2’s codemeta.json. &lt;otherEntity&gt; &lt;entityName&gt;CodeMeta file for BLE_LTER_CTD_QAQC.Rmd&lt;/entityName&gt; &lt;entityDescription&gt;CodeMeta file for annotated RMarkdown script to process, calibrate, and flag raw CTD data.&lt;/entityDescription&gt; &lt;physical&gt; &lt;objectName&gt;codemeta.json&lt;/objectName&gt; &lt;size unit=&quot;byte&quot;&gt;702&lt;/size&gt; &lt;authentication method=&quot;MD5&quot;&gt;8547b7a63abc6c1f0913a5bd7549d9d1&lt;/authentication&gt; &lt;dataFormat&gt; &lt;externallyDefinedFormat&gt; &lt;formatName&gt;application/json&lt;/formatName&gt; &lt;/externallyDefinedFormat&gt; &lt;/dataFormat&gt; &lt;/physical&gt; &lt;entityType&gt;CodeMeta&lt;/entityType&gt; &lt;/otherEntity&gt; For named projects, also include the software name, and also the version if applicable. The example below shows some additional metadata you can include. See also the more complete codemetar example and the available CodeMeta terms. Example 4: A more complete CodeMeta example for named projects. Example taken from the CodeMeta project Github with edits for brevity. { &quot;@context&quot;: [&quot;https://doi.org/10.5063/schema/codemeta-2.0&quot;, &quot;http://schema.org&quot; ], &quot;@type&quot;: &quot;SoftwareSourceCode&quot;, &quot;name&quot;: &quot;codemetar: Generate &#39;CodeMeta&#39; Metadata for R Packages&quot;, &quot;description&quot;: &quot;A JSON-LD format for software metadata&quot;, &quot;author&quot;: [{ &quot;@type&quot;: &quot;Person&quot;, &quot;givenName&quot;: &quot;Carl&quot;, &quot;familyName&quot;: &quot;Boettiger&quot;, &quot;email&quot;: &quot;cboettig@gmail.com&quot;, &quot;@id&quot;: &quot;https://orcid.org/0000-0002-1642-628X&quot; }, { &quot;@type&quot;: &quot;Person&quot;, &quot;givenName&quot;: &quot;Maëlle&quot;, &quot;familyName&quot;: &quot;Salmon&quot;, &quot;@id&quot;: &quot;https://orcid.org/0000-0002-2815-0399&quot; } ], &quot;codeRepository&quot;: &quot;https://github.com/ropensci/codemetar&quot;, &quot;dateCreated&quot;: &quot;2013-10-19&quot;, &quot;license&quot;: &quot;https://spdx.org/licenses/GPL-3.0&quot;, &quot;version&quot;: &quot;0.1.8&quot;, &quot;programmingLanguage&quot;: { &quot;@type&quot;: &quot;ComputerLanguage&quot;, &quot;name&quot;: &quot;R&quot;, &quot;version&quot;: &quot;3.5.3&quot;, &quot;url&quot;: &quot;https://r-project.org&quot; }, &quot;softwareRequirements&quot;: [{ &quot;@type&quot;: &quot;SoftwareApplication&quot;, &quot;identifier&quot;: &quot;R&quot;, &quot;name&quot;: &quot;R&quot;, &quot;version&quot;: &quot;&gt;= 3.0.0&quot; }, { &quot;@type&quot;: &quot;SoftwareApplication&quot;, &quot;identifier&quot;: &quot;git2r&quot;, &quot;name&quot;: &quot;git2r&quot;, &quot;provider&quot;: { &quot;@id&quot;: &quot;https://cran.r-project.org&quot;, &quot;@type&quot;: &quot;Organization&quot;, &quot;name&quot;: &quot;Comprehensive R Archive Network (CRAN)&quot;, &quot;url&quot;: &quot;https://cran.r-project.org&quot; } } ], &quot;keywords&quot;: [&quot;metadata&quot;, &quot;codemeta&quot;, &quot;ropensci&quot;] } Metadata to enable reproducibility Aside from the software/code itself and its dependencies, other pieces of information may be important should a user wish to reproduce results, such as the operating system and version and the system locale. Include this information in the data package’s methods/methodStep/description. For certain tools, there are ways to easily generate this information, e.g., a call to sessionInfo() in the R console. If the system outputs this information in a standardly formatted plain text file, that might be included as an otherEntity. 2.2.3 Linking between code and data There are a few solutions for providing explicit machine-readable linkages between different entities/packages (the distinction between code/data doesn’t matter too much here). For most cases we recommend the simplest approach, which is to use the methods/methodStep/description element of EML. More advanced users may wish to utilize the other solutions described herein. Descriptive approach In the dataset methods/methodStep/description element, include verbal descriptions such as “results.csv was derived from raw_data.csv using script.R” and repeat for all entities. If code and data reside in different packages, be sure to specify that. The EML dataSource element Nested under methods/methodStep, dataSource elements describe other data packages that serve as source for the current package. dataSource looks like a mini-EML tree describing the source data. Example: ecocomDP packages list the original packages under dataSource. dataSource does not describe relationships between entities in the same package, and as far as we know there is no explicit way in EML to do so. ProvONE ProvONE is a model developed by DataONE affiliates for provenance or denoting relationships between data entities. Each package on DataONE is described by a science metadata document (e.g., EML, ISO, FGDC) and a resource map document following ProvONE. The resource map powers a nice display of data relationships (see this package on the Arctic Data Center). This handles both relationships between entities in the same package and entities residing in different packages. However, note that EDI currently does not utilize this model. 2.2.4 External software Large community-backed tools or proprietary software such as ArcGIS or Microsoft Excel do not need to be archived. However, if they have had any impact on the final data (e.g., ArcGIS was used to modify spatial rasters), the EML methods section should describe the routines performed. Within the data package, indicate linkage to external software as follows. Briefly describe the software/code and its relationship to the data in EML’s methods/methodStep/description element. Names of all software used. Include both the common acronym and the full spelling. The URL(s) to all models/software used. Stable, persistent URLs pointing to exact version(s) are preferable, rather than generic links, e.g. to a project homepage. If the archived model has a DOI, then include a full citation to the model in the methods/methodStep/description text. Only exception to this is tools such as Excel that have achieved global household name status. Broadly, the system setup used (see section 1.1.b). Information on exact versions for all code used (including dependencies). This is important, e.g., ArcGIS Pro 2.4.1 is very different from ArcGIS for Desktop 10.7.1. Different systems have methods to easily generate this information, e.g. a call to sessionInfo() in the R console. Consider, if applicable, to archive the “runfile” as its own data entity within the data package, i.e., the script(s) that sets parameters and/or calls on functions imported from external software. Example 5: EML method description referring to external software. &lt;methods&gt; &lt;methodStep&gt; &lt;description&gt; &lt;para&gt; The seagrass coverage raster was created in ArcGIS Pro (version 2.4.3, by Esri) using the IDW geoprocessing tool on sampling_points.csv with a power of 2 and the nearest 12 points. &lt;/para&gt; &lt;para&gt; The raster was then refined using the seagrass-refiner package with the auto-refine option checked (Smith, 2017). &lt;/para&gt; &lt;para&gt; Smith, J. (2017). seagrass-refiner: a package that does the cool seagrass stuff. Version 1.2. https://doi.org/this-is/a-fake-doi, 2017. &lt;/para&gt; &lt;/description&gt; &lt;/methodStep&gt; &lt;/methods&gt; 2.3 Resources CodeMeta website CodeMeta crosswalks for a number of popular software CodeMeta terms you can use for describing software A description of some software licenses Best practices document to archiving model-based datasets "],
["model-based-datasets.html", "3 Model Based Datasets 3.1 Introduction 3.2 Recommendations for data packages 3.3 Resources", " 3 Model Based Datasets Contributors: An Nguyen, Tim Whiteaker, Corinna Gries 3.1 Introduction This document includes recommendations for archiving data packages comprised of model based datasets. These datasets may include the model code itself, input data, model parameter settings, and output data. This document was written for the intended audience of a Long Term Ecological Research (LTER) information manager (IM), though it is applicable to anyone handling model data in the context of a larger ecological research program. 3.1.1 Background The range of cases for model based datasets includes small one-off model code specific to one research question, through various code packages which are maintained in community repositories as long as they meet requirements (e.g., CRAN for R packages), to large community models maintained by groups of programmers and users. The intention of these recommendations is to make research based on modeling more transparent rather than achieve exact reproducibility, i.e., provide sufficient documentation so that a knowledgeable person can understand algorithms, programming decisions, and their ramifications for the results, rather than run the model and obtain the same results. It is not always easy to determine who among project personnel (IMs, scientists, programmers) is responsible for the different components of a model based dataset. This is best decided on a case-by-case basis. A common division is that the code authors annotate the code, and the IM handles the archiving and linkage to data product(s); partially except in cases of large community models (see section 2.3). The Environmental Data Initiative (EDI) is used as the representative data repository, however, the same practices could be applied to other similar repositories. A distinction is made between primarily data-oriented repositories (e.g., EDI), and specialized code repositories. 3.2 Recommendations for data packages 3.2.1 Referencing models in EML For data packages related to a model, whether the model is archived within the same data package or not, indicate linkage to the model in EML following Best practices for archiving software/code (section 1.2 and 2 in that document). Example EML snippet relating data to models via the method description: &lt;methods&gt; &lt;methodStep&gt; &lt;description&gt; &lt;para&gt; The seagrass coverage raster was created in ArcGIS Pro (version 2.4.3, by Esri) using the IDW geoprocessing tool on sampling_points.csv with a power of 2 and the nearest 12 points. &lt;/para&gt; &lt;para&gt; The raster was then refined using the seagrass-refiner model with the auto-refine option checked (Smith, 2017). &lt;/para&gt; &lt;para&gt; Smith, J. (2017). seagrass-refiner: a package that does the cool seagrass stuff. Version 1.2. https://doi.org/this-is/a-fake-doi, 2017. &lt;/para&gt; &lt;/description&gt; &lt;/methodStep&gt; &lt;/methods&gt; 3.2.2 Model code The model used to produce certain data needs to be well documented and linked from the resulting data product(s). However, it is not always easy to decide where and how to archive the code, and whether or not in conjunction with the data product(s). We outline in sections below three common code archiving options. Note that these scenarios (model code archived with data, or standalone in EDI, or elsewhere) are not mutually exclusive. Any project that involves code might make use of both established and custom software hosted on many different platforms, and might use some or all archiving options. To decide between archiving options, consider the questions listed in best practices for publishing software/code (see section 1). 3.2.2.1 Model code and data in the same package The goal of this practice is to ensure complete transparency of the data, and it applies to one-off models developed for the associated data, or occasionally to larger code bases for the reasons outlined best practices for publishing software/code (see section 1). Include the code as a dataset/otherEntity. Additionally, it is recommended to include a CodeMeta JSON-LD file, which can also be handled and documented in EML as dataset/otherEntity. CodeMeta is a metadata standard for software and code compatible with schema.org. Refer to best practices for publishing software for how to document the code and create CodeMeta. 3.2.2.2 Model code as standalone package If the model has been used to generate several datasets, i.e., is more widely applicable, it can be archived as its own package in EDI and assigned a DOI. Include the code as a dataset/otherEntity. Additionally, it is recommended to include a CodeMeta JSON-LD file, which can also be handled and documented in EML as dataset/otherEntity. CodeMeta is a metadata standard for software and code compatible with schema.org. Refer to best practices for publishing software for how to document the code and create CodeMeta. Consider including a list of derived data product(s) and their DOIs, ideally in the abstract of the EDI model package. This might lead to a chicken-and-egg issue, in which case we recommend publishing the model first, then including its DOI in the associated data packages(s). Then the IM might choose to take an extra step and push a revision to the model package to include the data DOIs. 3.2.2.3 Model code archived/maintained elsewhere This might include complex community models/software maintained by many people, published and actively maintained R/Python packages, etc. It may sometimes be advisable to archive a copy of the model code with the data, even if it appears to be maintained elsewhere. Refer to section 1 above to establish linkage from/to data products. 3.2.3 Model input and output data These are considered data entities, which should be handled according to EML best practices for corresponding data types. However, if the resulting datasets are very large, one may consider if input/output from all individual model runs need to be archived. Are there specific model run results that are more useful for non-modelers? For example: results from model runs leading to a journal publication. Very large model inputs/outputs may need to be archived offline. Refer to best practices for offline data. If the model requires a specific folder structure, you can zip model input files within the package to preserve that folder structure. A disadvantage of this approach is that you cannot elegantly describe each file with EML. 3.2.4 Model parameters Include model parameters whenever applicable. If code/input/output from multiple model runs are archived, make sure to archive all corresponding sets of parameters, and be explicit in linking the different components together. Consider archiving model parameter files as their own data object(s) in both their native format and as a text (non-binary) version. If the “runfile” will be archived, consider including the parameters within that file with appropriate annotations. 3.3 Resources Example model data packages already in EDI illustrative of different approaches: NTL LTER: General Lake Model (GLM) applied to Lake Mendota. EDI Portal. Parameters only. SBC LTER: Regional Ocean Modeling System (ROMS) applied to the California coast. EDI Portal. Inputs, code, parameters, outputs all zipped in one large file with presumably appropriate directory structure. Best practices for large datsets handled offline Best practices for publishing software "],
["still-images.html", "4 Still Images 4.1 Introduction 4.2 Recommendations for data packages 4.3 Resources", " 4 Still Images Contributors: Gastil Gastil-Buhl, Corinna Gries, Tim Whiteaker, Li Kui, Jason Downing, Greg Maurer, Renée Brown, Mark Servilla, Stace Beaulieu, Mary Martin, and An Nguyen 4.1 Introduction This document describes best practices for archiving still images, such as images that are taken and analyzed for a certain measurement, including but not limited to abundance, presence/absence, and coverage following the definition in the OBO ontology for ‘image’: “An image is an affine projection to a two dimensional surface, of measurements of some quality of an entity or entities repeated at regular intervals across a spatial range, where the measurements are represented as color and luminosity on the projected surface.” Examples include photos of sea ice taken from an automated camera station, images taken by plankton imaging systems, and images taken by an unmanned vehicle (underwater or aerial) for the above purpose. This best practice does not cover images that fall under the geospatial raster category such as satellite imagery, nor does it cover images already well handled in other systems, e.g., natural history specimen images, phenocam images. This document was written for the intended audience of a Long Term Ecological Research (LTER) information manager (IM), though it is applicable to anyone handling image data in the context of a larger ecological research program. We assume that the target archive is designed to handle ecological data, and that a given archive package will include metadata written in Ecological Metadata Language (EML) format. We refer to the Environmental Data Initiative (EDI) as an example archive. The same practices could be applied to other similar repositories. If large amounts of images (&gt;100GB in total) are to be archived, please coordinate with EDI before uploading. 4.2 Recommendations for data packages A still image data package includes the following: EML Metadata a CSV data table with meta information for each image, serving as an image catalog (optional) one or more image files or one or more zip files of the images as ‘otherEntity’ (optional) data tables with information extracted from the images (optional) data tables with other environmental information measured during image acquisition An image data package may contain image files individually (small numbers, up to ~10) or bundled as a zip archive. The decision of how to bundle images into zip archives and then into data packages should be guided by the overall goal of making image data usable for the intended purpose of the images. In most cases this involves finding specific images by, e.g., time or location of acquisition or some other aspect of interest. Hence, a balance has to be achieved between packages that are either too large or too numerous to be handled by a user. In addition, the effort of documenting images (each individually or in groups) has to be taken into account. The same decisions need to be applied within a data package in the design of zip archives. Images may be grouped by space and/or time. E.g., a stationary camera’s output may be archived in annual data packages, each of which may contain monthly zip files if the number of images is large. A moving camera’s output may also be archived in annual data packages, but individual zip files may hold all images for one transect. Another consideration is the timing of archiving. One should strive for archiving a fully processed group of images when no more changes or updates are expected due to the large volume to be handled repeatedly for each update. 4.2.1 Recommendations for Metadata Metadata are provided at two levels: in the EML file and in the csv file serving as an image catalog. Of course, all image data packages need the good discovery level metadata in EML (who, what, where, when, how, why). The detail provided on the level of the included csv image catalog (see below) should be guided by the same principles as stated above: to enable optimal usability of the images. E.g., images from a stationary camera need latitude and longitude only in the EML file not for every image. By contrast, images from a moving camera may need that information for every image or at least for every site/quadrat/transect. In addition to keywords describing the general purpose of the images (e.g., ice phenology, community composition, richness, etc.) it is recommended to include the keyword: Image with the semantic annotation from OBO: Term IRI: http://purl.obolibrary.org/obo/IAO_0000101** Definition: An image is an affine projection to a two dimensional surface, of measurements of some quality of an entity or entities repeated at regular intervals across a spatial range, where the measurements are represented as color and luminosity on the projected surface. 4.2.2 Image Catalog The image catalog is a required CSV data table with meta information for each image or each zip archive of a group of images. Each row in the catalog represents a single image or zip archive of a group of images. At a minimum, the table includes an attribute for the image/zip filename and additional attributes if other essential information varies per image/zip. Additional attributes may include essential information on the camera, date, time, creator, location, the image, the subject (required) Filename of each image or zip archive. If each image is described a relative path (directory structure) of the image including file extension, e.g., IMG_1001.jpg or 2018/SITE3/IMG1001.jpg should be included. Within this attribute in the image catalog, each value should be unique. (optional) Link/URL to download image if the image is available on a different system. (optional) Creator (optional) Datetime: Date and time associated with the image, in ISO 8601 format, e.g., 2007-04-05T12:30-02:00. Use the date and time that is important to the end user’s interpretation of the image. (optional) Location: Latitude and longitude in decimal degrees, site name, transect name, altitude, depth, habitat etc. (optional) all other conditions/descriptions that will help identify an image for use within the collection. Be consistent, use a controlled vocabulary for these fields, so that a user can search on them. Examples are: weather conditions, organism name, etc. (optional) Camera/instrument characteristics: model, settings, settings of a microscope (optional) Image characteristics: angle, distance, scale for pixel, resolution, magnification 4.2.3 Data Extracted from Images For data tables with information extracted from images (e.g., species abundance or coverage, or measurements that are made from image processing), whether included in the image data package or in a separate package, provide information to enable a user to locate the image from which a data value was extracted. This likely means including image filenames as a data column, or some other unique identifier or link if filenames cannot be used for this purpose. Or, if a given data row is derived from more than one image, consider including a cross reference table relating images to data rows. 4.3 Resources 4.3.1 Examples in EDI https://portal.edirepository.org/nis/mapbrowse?scope=knb-lter-jrn&amp;identifier=210011005 https://portal.edirepository.org/nis/mapbrowse?packageid=knb-lter-mcm.2016.2 https://portal.edirepository.org/nis/mapbrowse?scope=knb-lter-mcr&amp;identifier=5006 https://portal.edirepository.org/nis/mapbrowse?scope=knb-lter-mcr&amp;identifier=5013 "],
["genomic-and-metagenomic-data.html", "5 Genomic and Metagenomic Data 5.1 Introduction 5.2 Recommendations for Datasets 5.3 Resources", " 5 Genomic and Metagenomic Data Contributors: Hap Garritt, Tim Whiteaker, Sarah Elmendorf, An Nguyen, Gastil Gastil-Buhl, Renée Brown, Corinna Gries, and Li Kui 5.1 Introduction Genomic and metagenomic data that originate from gene sequencing techniques are most often stored in genomic repositories such as National Center for Biotechnology Information (NCBI) GenBank and the European Nucleotide Archive. Currently these data are not always easily discoverable in the context of other environmental data from the same project or research site. Here, we describe best practices to (a) allow discoverability using Ecological Metadata Language (EML) metadata; (b) archive ancillary data that are not included in the genomic repository; and (c) archive a data product derived from genomics data (without including the raw sequence data), while preserving access to original data in the associated genomic repository. This document was written for the intended audience of a Long Term Ecological Research (LTER) information manager (IM), though it is applicable to anyone handling genomics data in the context of a larger ecological research program. EDI is used as the representative data repository in this document. The same practices could be applied to other similar repositories. 5.1.1 Background Genomic data refers to the complete genetic information (either DNA or RNA) of an organism. Metagenomic data refers to the study of genomes recovered from environmental samples (water, soils etc.), i.e., genomes from multiple organisms. The sequence data consists of the order and arrangement of DNA bases (Adenine (A), Cytosine (C), Guanine (G), Thymine (T)) associated with individual species or multiple organisms from environmental samples. Genomic and metagenomic sequence datasets can be very large, include many sequence runs and typically are stored in associated community repositories that include specialized tools for searching, accessing, and analyzing (e.g., BLAST). Thus it is preferable that a specialized sequence database be the primary archive for the sequences themselves, rather than a generalist repository such as the Environmental Data Initiative (EDI). Sequence repositories assign unique identifiers to projects, samples and/or single sequences, often called accession numbers, which can be used to locate sequence data. There are numerous genomic databases. The investigator is typically responsible for sequence data submission to their repository of choice and most are familiar with the data submission process as it is often required for subsequent manuscript publication. Therefore, selection of a genomic repository and submission of genomic data to a genomic repository is outside the scope of this document, although the recommended data repositories for nucleic acid sequence data for the journal Scientific Data may provide some useful guidance (https://www.nature.com/sdata/policies/repositories#nuc). The key point here is that the genomic researcher should be in communication with the information manager about where data have been submitted including accession numbers and metadata important for discovery. This document discusses best practices for making these sequence data discoverable in the data repository (EDI) that holds related environmental measurements and in the context of a larger research site in general and in particular, when these additional measurements (of biochemistry, environmental conditions) were taken on the same samples that have been used for genetic analysis. Note that each genomic repository may have its own mechanism for reverse linking to such contextual data in EDI, and these mechanisms are beyond the scope of this document. Furthermore, a distinction should be made between raw/original sequence data and derived products such as metagenome-assembled genomes (MAGS) and operational taxonomic units (OTUs), which are dependent upon algorithms/post processing of raw data. These data products (frequently a community analysis) need to be archived in EDI. 5.1.2 Types of Genomics Datasets Commonly Encountered An information manager typically encounters one of four types of genomics datasets: Only sequence data Sequence data with ancillary environmental data already submitted to the genetic repository Sequence data with ancillary environmental data not yet archived Derived data product with or without ancillary environmental data 5.2 Recommendations for Datasets Figure 1. Decision tree and workflow for different cases of genetic sequence data The remainder of this document describes best practices in these sections: Metadata for all genomics datasets Genomics data summary Genomics data with ancillary data Derived data products 5.2.1 Metadata for All Genomics Datasets Archived in EDI Do be as complete as feasible with the metadata, including creator lists, maintenance, contacts, project description, and so on, as described in EML Best Practices. Include geographic coverage, temporal coverage, and taxonomic coverage if possible so that users can locate the dataset by space, time, and taxa. Guidance for specific fields is provided below. Dataset title - Follow general best practices for dataset titles. Consider using the same title as in the genomics repository if that title makes sense. Abstract - Study description giving rise to the genomics data. Indicate that more details are available in the genomics repository, and provide a link to the repository. You could Include links for related datasets here, e.g., for water or soil chemistry, but we recommend including such data in the same data package with the genomics data summary. Keywords - In addition to keywords typically used by the site, use keywords that help identify this as a genomics dataset, such as: genomics metagenomics target gene or subfragment (e.g. 16S rRNA, 18S rRNA, nif, amoA, rpo, ITS) the sequencing technique (e.g., Sanger, pyrosequencing, ABI-solid) Consider using keywords that the investigator supplied on genomics data submission templates, such as MIxS templates. the genomics repository name If the data package includes a summary table of what’s available at a genomics repository, include the keyword “data inventory” as per the best practices for data linked at other repositories. For genomics data with ancillary data, regardless of where they are archived, include relevant keywords such as: nitrogen soil moisture For derived data products, include relevant keyword such as OTU population community Dataset Methods - Describe sampling and sequencing methods here. Describe methods for collection and processing of ancillary data as appropriate. Literature - Take advantage of EML 2.2 capabilities for citing publications to reference papers associated with the genomics data. 5.2.2 Genomics Data Summary Every genomics dataset should contain at least one table with a summary or index of what is stored in a genomics repository. This enables all data from a particular genomics dataset to be discovered. Below are suggested attributes for this data table. Some attributes may not be appropriate for all datasets, and additional attributes could be provided (e.g., from MIxS templates) if available. Note that the granularity of data depends upon which database link is used for accession number from the genomics data repository. For example, in NCBI, there are accession numbers for a project, samples within the project, and sequence datasets from a given sample. The information manager should work with the genomics researcher to determine for which level of granularity accession numbers should be provided in related data packages in EDI. Attribute Description Sample_ID Generally the locally used identifier for the sample, accession numbers may be used instead Collection_date Date of sample collection Location_name Locally used name of collection site Latitude Latitude of sampling area in decimal degrees Longitude Longitude of sampling area in decimal degrees Genomics_repository Name of genomics repository Accession_number Range of accession numbers or single accession number for a study. Some studies have hundreds of accession numbers. Accession_link Accession number based URL for the dataset in the genomics repository, e.g., https://www.ncbi.nlm.nih.gov/nuccore/AY741555. If providing a range of accession numbers, you may have to be creative, for example, by providing a search URL that will return the desired list, e.g., https://www.ncbi.nlm.nih.gov/popset/?term=AY741555. Recommendation is to link to the widest level of granularity for which additional data useful to interpretation are ONLY archived on EDI. As an example, if an OTU table which is archived on EDI provides taxonomic identifications for many samples, it would be most useful for an end user if that table on EDI also contained the corresponding BioSample number from NCBI. However, if there are no derived products on EDI that exist on a per sample basis, then linking to the larger BioProject would be sufficient, because NCBI provides the child BioSample hierarchy under BioProject that will allow a user to discover all the data. seq_method Sequencing method used; e.g., Sanger, pyrosequencing, ABI-solid. This attribute is used in MIxS templates, where it is called seq_meth. internal_control_seq Internal control sequences used in the dataset, e.g., Thermus Thermophilus. If no internal control is added then you could leave you could use none or indicate as such iethods section in EML if the same applies to all records in the dataset. env_biome Biomes are defined based on factors such as plant structures, leaf types, plant spacing, and other factors like climate. Biome should be treated as the descriptor of the broad ecological context of a sample. Examples include: estuarine biome, marine coral reef biome, temperate deciduous broadleaf forest, or subtropical desert biome. EnvO (v 2013-06-14) terms can be found via the link: www.environmentontology.org/Browse-EnvO by searching for biome. This attribute is used in MIxS templates. env_feature Environmental feature level includes geographic environmental features. Compared to biome, feature is a descriptor of the more local environment. Examples include: harbor, cliff, or lake. EnvO (v 2013-06-14) terms can be found via the link: www.environmentontology.org/Browse-EnvO by searching for environmental feature. This attribute is used in MIxS templates. env_material The environmental material displaced by the sample, or material in which a sample was embedded, prior to the sampling event. Environmental material terms are generally mass nouns. Examples include: alluvial sediment or coastal sea water. EnvO (v 2013-06-14) terms can be found via the link: www.environmentontology.org/Browse-EnvO by searching for environmental material. This attribute is used in MIxS templates. Taxon_description If applicable, e.g., Genus species, or taxonomic group Reference_publication Title of publication providing in-depth context for data Publication_DOI Publication DOI 5.2.3 Genomics Data with Ancillary Data If the ancillary environmental data are to be archived in EDI, make sure a link to the genomics data is established. Include in your data table(s) the following columns at a minimum, which allow the user to identify the related genomics dataset: A column identifying the accession number for the related genomics data The full URL linking to the related genomics dataset The same sample ID as in the genomics data summary table, i.e., use the sample ID as a key field to relate records from one data table to another Note for sequence data: to align with the Darwin Core standard, you can use a column header “associatedSequences” (https://dwc.tdwg.org/terms/#dwc:associatedSequences) and populate each record with a unique identifier (or list concatenated and separated of identifiers) for the sequence data in the other repository (e.g., SNLBE002-17, a sequence in Barcode Of Life Data system, aka BOLD) or full URL (e.g., http://www.boldsystems.org/index.php/Public_RecordView?processid=SNLBE002-17). You can specify the genomics repository in the package’s metadata. For the case of more than one related repository, you can include a column identifying the data repository in the data table. You may include additional columns about the genomics data as needed. Many external genomics repositories allow ancillary data such as sample chemistry to be archived together with the sequence data. To avoid data duplication, best practices are to archive on EDI only data that have not been archived elsewhere. However, to enable discovery of these ancillary data on the genomics repository, we recommend including appropriate keywords and methods to indicate that environmental (or other, non-sequence) data are available in the related genomics dataset archive. 5.2.4 Derived data products Data products derived from genomics data frequently are community/population/trait datasets where species, OTUs or traits have been determined from the sequence data. In such datasets it is important to ensure the reliable connection back to the original genomics data via accession numbers. 5.3 Resources 5.3.1 Tips for locating metadata If information is not readily available for populating metadata in EML, they may be found in the genomics repository. For data in NCBI, go to theNCBI website and search using accession number or search by accession number in a specific NCBI Database, for example Genes PopSet (the PopSet database is a collection of related DNA sequences derived from population, phylogenetic, mutation and ecosystem studies that have been submitted to NCBI) and there will be lots of metadata that investigators provided when submitting data. For sequences submitted to the NCBI Sequence Read Archive, there are some easily accessible online tools for generating tables of linked sequence data and their metadata. For an example, go to the example dataset at https://www.ncbi.nlm.nih.gbov/bioproject/305753, and click the number next to SRA Experiments to see a list of all experiments. Then click Send results to Run selector to see a table summarizing geolocations and associated metadata which could be archived at EDI or used to extract metadata for EML preparation. 5.3.2 Links to various repositories NCBI Databases - list of various databases with search capabilities. See also How to submit data to GenBank. DNA DataBank of Japan (DDBJ) - list of various databases with search capabilities. See alsoSubmissions. European Nucleotide Archive (ENA) - list of various databases with search capabilities. See also Submit and update. Integrated Microbial Genomes &amp; Microbiomes (IMG/M) system from the Joint Genome Institute BCO-DMO examples for contributing sequence accession numbers. Barcode of Life DataSystems (BOLD) DNA barcoding is a taxonomic method, that uses one or more standardized short genetic markers in an organism’s DNA to identify it as belonging to a particular species. Through this method unknown DNA samples are identified to registered species based on comparison to a reference library. MG-RAST (technically an analysis pipeline not a primary repository, but replicates to primary repositories) Replicates to the European Bioinformatics Institute (EMBL-EBI), which in turn replicates to the NCBI Sequence Read Archive (such that data submitted on MG-RAST will automatically appear on all three) NCBI Accession Number prefixes 5.3.3 Related Best Practices and Templates EML Best Practices v3 Data Package Best Practices on GitHub (includes latest draft of EML Best Practices) Minimum Information about any (x) Sequence (MIxS) templates "],
["large-data-sets.html", "6 Large Data Sets 6.1 Introduction 6.2 Recommendations for data packages 6.3 References 6.4 Potential Issues", " 6 Large Data Sets Contributors: Margaret O’Brien, Corinna Gries, Mark Servilla 6.1 Introduction Data entities are kept offline when they are too large to be handled easily by the HTTP protocol, are expected to be rarely requested, and can be mailed on an external drive. If you suspect your data fall into this category, contact EDI for advice (support@environmentaldatainitiative.org). Below are recommendations for the EDI repository’s handling of data packages that have an offline component. 6.1.1 Background Standard practice is to handle data entities (both upload and download) via the HTTP protocol, using a URL. However, for very large datasets HTTP can fail due to physical limits. The limit for “too large” is somewhat subjective; EDI’s current limit for datasets that are “too large for HTTP” is 100GB (all data and metadata). 6.2 Recommendations for data packages 6.2.1 Physical Storage The use of a Solid-state Drive (SSD) is strongly recommended for all offline data storage. The SSD should be formatted using one of the following file systems: 1) exFAT, 2) NTFS, or 3) ext4. Each of these file systems can accommodate individual file sizes greater than 1TB. Add data to external drive in native (non-compressed, non-tarred, non-zipped) format, deliver to EDI (e.g., by physical mail). EDI will store three copies, one external hard drive each in New Mexico and in Wisconsin, one copy in general EDI backup cloud storage. Please mail one copy each to: Attn: Mark Servilla UNM Biology, Castetter Hall 1480 MSC03-2020, 219 Yale Blvd NE Albuquerque, NM 87131-0001 Attn: Corinna Gries University of Wisconsin Center for Limnology 680 North Park Street Madison WI 53706-1413 6.2.2 Data package The external hard drive should contain at least two entities: the data (which will be offline) and an inventory or manifest that describe the contents of the external hard drive. Content of the manifest (inventory of holdings) would be dictated by the type of data entity. The manifest will be available as an online entity (through the EDI Data Portal) so that potential requestors can evaluate the offline resource before requesting it. Suggested columns are: Filename(s) Format (netCDF, tabular csv, etc.) Start_datetime End_datetime Location_lat Location_lon (other params the PIs may feel are essential) Checksum 6.2.3 Package Metadata (in EDI metadata template and converted to EML - generally, as for any data package) Abstract: describe the collection generally. If individual files require specific software to read, provide the name of that software. Creators Contact (will be responsible for sending out copies as requested.) positionName: EDI Repository Manager Email: support@environmentaldatainitiative.org Methods - detailed collection/generation methods for the offline data entities. Detailed information for re-using the data. (May instead be included in the manifest table if different for different offline files.) Data Entities Offline Entity: Describe as you would for an online resource. Restate the software needed to read the individual files if this is important to a user. See Table 1 and Sample XML. Manifest (inventory of the offline holdings) Column descriptions as for any data table 6.2.4 EML In addition to basic resource-level metadata, at least two entities should be described: Manifest (inventory) should be a tableEntity: will be the online entity and described as all Offline entity: Fill out high-level fields as for an online resource. Restate the software needed to read the individual files if this is important to a user. Distribution node will be offline (See Table 1, code block) 6.2.4.1 Table 1. Three required fields for an offline distribution physical/objectName As for any entity, this is the name of the file or data object dataFormat/ExternallyDefinedFormat/formatName The name of the format the data object is in. If there is a special compression applied, list it here. distribution/offline/mediumName Instead of a data URL, you will have an offline distribution node. The name of almost all offline media is “external drive”, because that is how you will deliver the data to a requestor. 6.2.4.2 Sample XML, offline entity &lt;physical&gt; &lt;objectName&gt;mainl_2005acc.zip&lt;/objectName&gt; &lt;dataFormat&gt; &lt;externallyDefinedFormat&gt; &lt;formatName&gt;netCDF file&lt;/formatName&gt; &lt;/externallyDefinedFormat&gt; &lt;/dataFormat&gt; &lt;distribution&gt; &lt;offline&gt; &lt;mediumName&gt;External drive&lt;/mediumName&gt; &lt;/offline&gt; &lt;/distribution&gt; &lt;/physical&gt; 6.3 References 6.3.1 EML documentation https://eml.ecoinformatics.org/schema/index.html Look for the PhysicalDistributionType 6.4 Potential Issues SSD formatting (eventually, whatever we use, it will become unusable). Even with cloud storage, eventually a binary format will become unusable. "],
["data-in-other-repositories-1.html", "7 Data in Other Repositories 7.1 Introduction 7.2 Considerations for creating linked repository records 7.3 Recommendations for Data Packages 7.4 Example data packages", " 7 Data in Other Repositories Contributors: Greg Maurer, Margaret O’Brien, Corinna Gries, Tim Whiteaker, Sarah Elmendorf 7.1 Introduction A wide variety of data repositories are available to ecologists for publishing their data. The choice of where to publish a dataset is determined by many factors. The funding agency or a journal may require a certain repository (e.g., NSF BCO-DMO, NSF ADC, USDA ADC, DOE ESS-DIVE); the research subject may be best supported by a very specialized repository (e.g., AmeriFlux, GenBank); or the data were originally submitted to a general purpose repository with minimal metadata requirements (e.g., DRYAD, Figshare, Zenodo). To enhance data discoverability and interoperability, users and data managers may find it helpful to create additional repository records that are linked to these published datasets. These concepts must be understood to clarify this best practices document: A dataset consists of a data file, or files, containing named variables (say “Soil_T_10cm”) with assigned values (the temperature numbers themselves) AND the accompanying metadata describing attributes of the data and other information. A duplicate dataset contains data and/or metadata that are identical to those in another public data repository. A linked dataset contains metadata, but the only data present are links to the unique identifier(s) of data held in another repository. For the purposes of this document, all three dataset types above are published in online research data repositories and are assigned unique digital identifiers (DOIs). This document discusses reasons for creating additional repository records for a published dataset, i.e., linked datasets pointing to other repositories, and recommendations of how to develop such records. It was written for the intended audience of a Long Term Ecological Research (LTER) information manager (IM), though it is applicable to anyone handling data in the context of a larger ecological research program. EDI is used as the representative data repository in this document. The same practices could be applied to other similar repositories. 7.2 Considerations for creating linked repository records Data managers and users have several reasons to consider creating linked datasets, i.e. additional repository records linked to a published dataset. For the LTER IM, this practice involves creating a new data package in the EDI repository that links to data already published in another (non-EDI) repository. However, care should be taken to avoid duplicating the same data in multiple repositories. The practice of duplicating research data in multiple repositories is discouraged because a) this creates a burden of maintaining multiple datasets to avoid divergence between them, and b) it creates confusion for data re-users who may be led to download the same data multiple times. There are clear cases in which creating linked datasets offers benefits to data managers and users. There are also cases in which this practice should be avoided. Both types of cases are described in the sections below, but there are situations in which the pros and cons of this practice will need to be weighed. Experience with data user behavior and support is paramount for such decisions and may be made differently by different research groups. 7.2.1 Cases supporting linked datasets Requirements dictate multiple repositories: Large research projects or sites are frequently funded by different agencies and programs. Data collection may be supported by several such funding streams and, hence, fall in the purview of more than one requirement for data being archived in a particular repository. In some cases, data repositories already accommodate such requirements by linking or replicating data appropriately. Best examples are LTER data in EDI, NSF BCO-DMO and NSF ADC. Improved discoverability: Data from a large research project or sites become better discoverable when published and, hence, discoverable within the same repository. This practice also puts the data into the larger context of all research and ancillary data at that location aiding in interpretation and analysis. It, furthermore, allows the site to easily build a more complete data catalog for reporting or other administrative purposes without having to pull data citations from different sources. By the same token, the same type of data from disparate projects or sites may be more discoverable and easier to integrate when stored in a specialized repository focused on a particular type of data. For instance, specialized data repositories can enforce data formatting or quality standards and require metadata elements that enhance search, discovery and reuse of particular types of data across projects in a way that is not possible using a generalized metadata form (EML) and repository (EDI). Adding important metadata: If data were originally submitted to a general purpose repository with minimal metadata requirements (e.g., Dublin core) additional metadata (e.g., EML) may be needed for discoverability, reusability and integration. By creating a new repository record linked to the published dataset, richer and more useful metadata can be added and utilized. Derived data products: In many cases raw data are best handled in a specialized repository, but derived data products based on those raw data are, or should be, published in a different repository. For example, genomics data such as raw sequencing data are best archived in a specialized repository like NCBI GenBank, but derived products such as trait data or diversity metrics would be most useful in EDI (see the Genomics best practices document for more). The same principles apply whenever a project or researcher has developed specialized procedures to process raw data or compute aggregations that are more appropriate for most users. In these cases it is important to maintain links between the derived data and the raw data when they are in separate repositories. 7.2.2 When to avoid linked datasets Site- or Project-relevant data from outside research groups or agencies: Although it may help with some aspects of data discovery it is generally not recommended to create linked datasets (or duplicates) for data collected and managed by entirely different research groups or agencies. In these cases, however, it is recommended to place a pointer to such repositories on a project website. Linking between repositories that are DataONE member nodes: The DataONE aggregator provides access to earth data held in numerous repositories through a unified interface. It is not recommended to duplicate this functionality by creating additional links between repositories. 7.3 Recommendations for Data Packages When the decision is made that a dataset needs to be accessible in more than one repository the following considerations should guide the process: The metadata need to clearly state the original location of the data Assigning more than one unique identifier (e.g., DOI) to the same data can create confusion when data is cited. Thus, the DOI of the data itself should be clearly stated in the linked data package. The original repository should receive appropriate credit One data package that functions as an inventory of data in other repositories for a site is an option, but will not improve discoverability of single datasets. In EDI, the linked data package can be assembled using standard practices and EML metadata elements, but the included data entities must clearly lead the data user to files held in outside repositories. In addition, the package metadata should communicate the essential elements needed for data discovery (subject matter, authors, location, time-frame, etc.) and a brief description of how the data may be accessed and re-used via the outside repository. Metadata elements to be created in EML, and contents of the included data entities are described below. 7.3.1 Package Metadata The EML metadata that will accompany the data package in EDI must contain the essential information for users to discover and use the data at the outside data repositories. Abstract: Describe the collection generally. Since the actual data are held elsewhere, be sure to describe the location of the data being linked to. Also provide sufficient information for users to find and re-use the data entities at the outside data repository. Creators Contacts Methods: Collection/generation methods for the data entities at the linked repository. If the methods are well-described at the outside repository being linked to, this element can simply refer users there. In addition, instructions for data access at the linked data repository should be provided. Geographic description and coordinates: At a minimum these elements should define a bounding box that will make the data package discoverable through EDI, DataOne, or other geographic search interfaces. Additional coordinates can be given in the manifest entity below. Keywords: If this is an inventory rather than a single dataset duplication held at a different repository, include the keyword &quot;data inventory&quot; and thematic keywords that describe the data entities in the other repository. Data Entity: This is most likely a simple tabular manifest (inventory of holdings) that describes the data entities held in another data repository (or repositories). This should be a dataTable entity, described as you would for any dataTable with regular column descriptions. In rare cases the data themselves may be duplicated, though this is only recommended if additional metadata must be added to interpret or enrich the data. 7.3.2 Contents of the manifest dataTable entity Any data package that links to data held in an outside repository should contain at least one data entity: a manifest (inventory) of data holdings at a linked repository(ies). The manifest should be written in a tabular data format (such as a .csv) that can be described as a dataTable entity in EML. For the specific case of genomics data see here. For each outside data holding, the manifest (dataTable entity) should have a row that includes persistent links such as DOIs to any data entities at the linked repository, relevant package identifiers, and descriptors for the data entity. The complete content of the manifest (inventory of holdings) would be dictated by the structure of the linked repository and the type of data entities there. Suggested columns are: Table 1: Required columns in the manifest entity. Column name Description Filename(s) Filename at the repository being linked to Format File format of above Title/description Title and/or brief description of the data resource External package/entity ID Unique identifier at the repository being linked to. Publication_DOI DOI of of data published at the repository being linked to Repository_URL URL of the repository being linked to Table 2: Optional columns in the manifest entity. These may be included in the manifest when needed to describe multiple data entities. Column name Description Latitude Latitude in decimal degrees, WGS84 datum, associated with the data resource. For bounding boxes, use North_latitude and South_latitude columns. Longitude Longitude in decimal degrees, WGS84 datum, associated with the data resource. For bounding boxes, use East_longitude and West_longitude columns. Start_datetime Starting datetime of the linked data file End_datetime Ending datetime of the linked data file (NA if data collection is ongoing) 7.4 Example data packages Each of the EDI packages below contain dataTable entities with links to outside repositories. These dataTables serve as the manifests (inventory) for the externally-held data described in the rest of the metadata. Table 3. Linked-repository packages at EDI Title Description EDI packageID Mass and energy fluxes from the US-Jo2 AmeriFlux eddy covariance tower in Tromble Weir experimental watershed at the Jornada Basin LTER site, 2010-ongoing This data package links to eddy covariance data from a Jornada Basin LTER tower. The data are held at the AmeriFlux data repository (https://ameriflux.lbl.gov) knb-lter-jrn.210338005 "]
]
